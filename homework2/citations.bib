@online{noauthor_variational_2023,
	title = {Variational autoencoder},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/wiki/Variational_autoencoder},
	abstract = {In machine learning, a variational autoencoder ({VAE}), is an artificial neural network architecture introduced by Diederik P. Kingma and Max Welling, belonging to the families of probabilistic graphical models and variational Bayesian methods.Variational autoencoders are often associated with the autoencoder model because of its architectural affinity, but with significant differences in the goal and mathematical formulation. Variational autoencoders are probabilistic generative models that require neural networks as only a part of their overall structure. The neural network components are typically referred to as the encoder and decoder for the first and second component respectively. The first neural network maps the input variable to a latent space that corresponds to the parameters of a variational distribution. In this way, the encoder can produce multiple different samples that all come from the same distribution. The decoder has the opposite function, which is to map from the latent space to the input space, in order to produce or generate data points. Both networks are typically trained together with the usage of the reparameterization trick, although the variance of the noise model can be learned separately.
Although this type of model was initially designed for unsupervised learning, its effectiveness has been proven for semi-supervised learning and supervised learning.},
	booktitle = {Wikipedia},
	date = {2023-05-17},
	langid = {english},
	author = {Wikipedia}
}
@online{patacchiola_evidence_2021,
	title = {Evidence, {KL}-divergence, and {ELBO}},
	url = {https://mpatacchiola.github.io/blog/2021/01/25/intro-variational-inference.html},
	abstract = {A blog series about Variational Inference. This post introduces the evidence, the ELBO, and the KL-divergence.},
	language = {en},
	urldate = {2023-06-03},
	journal = {mpatacchiola's blog},
	author = {Patacchiola, Massimiliano}
}

@online{noauthor_understanding_nodate,
	title = {Understanding the {Variational} {Lower} {Bound}},
	url = {https://xyang35.github.io/2017/04/14/variational-lower-bound/},
	author = {Xitong Yang},
}

@online{JianlinSu_nodate,
	title = {Variational Auto-Encoder-1{\textbar}Scientific Spaces},
	url = {https://spaces.ac.cn/archives/5253},
	author = {Jianlin Su}
}

@online{weng_policy_2018,
	title = {Policy Gradient Algorithms},
	url = {https://lilianweng.github.io/posts/2018-04-08-policy-gradient/},
	abstract = {[Updated on 2018-06-30: add two new policy gradient methods, {SAC} and D4PG.]  [Updated on 2018-09-30: add a new policy gradient method, {TD}3.]  [Updated on 2019-02-09: add {SAC} with automatically adjusted temperature].  [Updated on 2019-06-26: Thanks to Chanseok, we have a version of this post in Korean].  [Updated on 2019-09-12: add a new policy gradient method {SVPG}.]  [Updated on 2019-12-22: add a new policy gradient method {IMPALA}.},
	author = {Weng, Lilian},
	urldate = {2023-06-04},
	date = {2018-04-08},
	langid = {english},
}

@book{richard_s_sutton_reinforcement_2022,
	edition = {Second Edition},
	title = {Reinforcement Learning: An Introduction},
	url = {http://incompleteideas.net/book/the-book.html},
	publisher = {{MIT} Press},
	author = {{Richard S. Sutton} and {Andrew G. Barto}},
	urldate = {2023-06-04},
	date = {2022-04-26},
}