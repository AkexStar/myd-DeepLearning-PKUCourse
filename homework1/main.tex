\documentclass{homework}
% \usepackage[UTF8]{ctex}
\usepackage{amsmath, amsthm, amssymb, bm, color, framed, graphicx, mathrsfs}

\author{Li-Jintao 2201213292 lijintao@stu.pku.edu.cn}
\class{Prof. Mu-Yadong's Deep Learning: Algorithms and Applications}
% \class{Prof. 穆亚东: 深度学习算法与应用}
\date{May 10th, 2023}
% \title{\Large \textbf{第一次课程作业\quad Homework-1}}
\title{\Large \textbf{Homework-1}}
% \address{Remote Sensing Building 318, Peking University, Beijing}
\definecolor{shadecolor}{RGB}{241, 241, 255}
\graphicspath{{./media/}}

\begin{document} \maketitle
\rule[0ex]{\textwidth}{1.5pt}
\begin{shaded}
    \question\textbf{MLP for Classification: }
In a multi-class classification problem, e.g. a problem with $M$ classes, we use a model $f_{\theta}$ to produce the output $\hat{y}$. $\hat{y}$ is a vector of dimension $M$ with sum 1, whose components indicate the probability of $x$ belong to each class.

In this section, you will use a simple model -- the 3-layer MLP -- to tackle this problem. Assume that we have $K$ labeled data $\left(x_{k}, y_{k}\right)$ with $k=1,2, \ldots, K$ where $x_{k} \in \mathbb{R}^{N}$ and $y_{k} \in \mathbb{R}^{M}$ is a $M$ dimensional one-hot vector. 
\begin{enumerate}
    \item[a)] Define the cross entropy loss for a multi-class classification problem.
    \item[b)] A 3-layer MLP consists of an input layer, a hidden layer, and an output layer with two learned parameter matrices $W^{1}, W^{2}$ between successive layers. Please note that there is generally a Softmax layer after the output layer to scale the output, which we omitted in this sub-question for simplicity. When given an input $x$, this model performs the following forward computations sequentially:
    \[\begin{aligned}
    & a_{1}={\rm W^{1}} x, \\
    & h=\sigma\left(a_{1}\right), \\
    & a_{2}=\mathrm{ W^{2}} h, \\
    & \hat{y}=\sigma\left(a_{2}\right) .
    \end{aligned}\]
    where ${\rm W^{1}} \in \mathbb{R}^{D \times N}, \mathrm{W^{2}} \in \mathbb{R}^{M \times D}$ are parameter matrices and $\sigma(z)=\frac{1}{1+e^{-z}}$ is the element-wise Sigmoid function. Use the loss function you defined in sub-question a), apply an SGD update to the parameters $\rm W^{1}$ and $\rm W^{2}$ with learning rate $\eta$ via backpropagation. Must identify the closed-form gradient of each parameter, which is $\frac{\partial \mathcal{L}}{\partial \mathrm{W^{2}}(m, d)}$ and $\frac{\partial \mathcal{L}}{\partial \mathrm{W^{1}}(d, n)}$ in your derivation.
\end{enumerate}
\textbf{HINT:}
    1) For simplicity, you can assume that the SGD program uses only one training data per batch. 
    2) Use formula $\sigma^{\prime}(z)=\sigma(z)(1-\sigma(z))$ for a scalar $z$. 
\end{shaded}
\textbf{SOLUTION:}

\textbf{a)} The cross-entropy loss for a multi-class classification problem is defined as:
\[\mathcal{L}=-\frac{1}{B} \sum_{b=1}^{B} {y}_{b}^{T} \log {\hat{y}}_{b}\]
where $B$ stands for the batch-size of training data, and $y_{b}$ is the $M$ dimensional one-hot vector for the $b$-th training example, and $\hat{y}_{b}$ is the output vector predicting the $b$-th example. 

According to the \textbf{HINT}, we assume that the SGD program uses only one training data per batch. Then we have $B=1$, and the loss is:
\[\mathcal{L}={y}^{T} \log {\hat{y}}\]

\textbf{b)} To compute the gradients of the loss function with respect to $\rm W^1$ and $\rm W^2$ using back-propagation and SGD, we first need to compute the derivative of the loss function with respect to $\hat{y}(m)$, where $m \in \{1,2,\ldots,M\}$ denotes the $m$-th neuron in the output layer. And it is given by:
\[\frac{\partial \mathcal{L}}{\partial \hat{y}(m)}=-\frac{y(m)}{\hat{y}(m)}\]

Using the chain rule, we can compute the derivative of the loss function with respect to $a_2$:
\begin{align*}
    \frac{\partial \mathcal{L}}{\partial a_{2}(m)}
    &=\frac{\partial \mathcal{L}}{\partial \hat{y}(m)} \frac{\partial \hat{y}(m)}{\partial a_{2}(m)}\\
    &=-\frac{y(m)}{\hat{y}(m)}\sigma(a_{2}(m))\left(1-\sigma(a_{2}(m))\right)\\
    &=y(m)\left(\hat{y}(m)-1\right)
\end{align*}

Using the same chain rule, we can compute the derivative of the loss function with respect to $a_1$:
\begin{align*}
    \frac{\partial \mathcal{L}}{\partial a_{1}(d)}
    &=\sum_{m=1}^{M}\frac{\partial \mathcal{L}}{\partial a_{2}(m)}\frac{\partial a_{2}(m)}{\partial h(d)}\frac{\partial h(d)}{\partial a_{1}(d)}\\
    &=-\sum_{m=1}^{M}\frac{y(m)}{\hat{y}(m)}\sigma(a_{2}(m))\left(1-\sigma(a_{2}(m))\right)\mathrm{W^{2}}(m,d)\sigma(a_{1}(d))\left(1-\sigma(a_{1}(d))\right)\\
    &=\sum_{m=1}^{M}y(m)\left(\hat{y}(m)-1\right)\mathrm{W^{2}}(m,d)h(d)(1-h(d))
\end{align*}

Finally, we can compute the gradients of the loss function with respect to the parameter matrices:
\begin{align*}
    \frac{\partial \mathcal{L}}{\partial \mathrm{W^{2}}(m,d)}
    &=\frac{\partial \mathcal{L}}{\partial \hat{y}(m)} \frac{\partial \hat{y}(m)}{\partial a_{2}(m)}\frac{\partial a_{2}(m)}{\partial \mathrm{W^{2}}(m,d)}\\
    &=\frac{\partial \mathcal{L}}{\partial a_{2}(m)}\frac{\partial a_{2}(m)}{\partial \mathrm{W^{2}}(m,d)}\\
    &=-\frac{y(m)}{\hat{y}(m)}\sigma(a_2(m))(1-\sigma(a_2(m)))h(d)\\
    &=y(m)(\hat{y}(m)-1)h(d)
\end{align*}
\begin{align*}
    \frac{\partial \mathcal{L}}{\partial \mathrm{W^{1}}(d, n)} 
    &= \frac{\partial \mathcal{L}}{\partial \hat{y}(m)} \frac{\partial \hat{y}(m)}{\partial a_{2}(m)} \frac{\partial a_{2}(m)}{\partial h(d)} \frac{\partial h(d)}{\partial a_{1}(d)} \frac{\partial a_{1}(d)}{\partial \mathrm{W^{1}}(d, n)} \\
    &= -\sum_{m=1}^{M}\frac{y(m)}{\hat{y}(m)}\sigma(a_2(m))(1-\sigma(a_2(m))) \mathrm{W^2}(m,d)\sigma(a_1(d))(1-\sigma(a_1(d)))x(n)\\
    &= \sum_{m=1}^{M} y(m)\left(\hat{y}(m)-1\right)\mathrm{W^2}(m,d)h(d)(1-h(d))x(n)
\end{align*}

Therefore, the update rule for the parameters $\rm W^2$ and $\rm W^1$ are:
\begin{align*}
\mathrm{W^1}(d, n) &\leftarrow \mathrm{W^1}(d, n) - \eta \sum_{m=1}^{M} y(m)\left(\hat{y}(m)-1\right)\mathrm{W^2}(m,d)h(d)(1-h(d))x(n)\\
\mathrm{W^2}(m, d) &\leftarrow \mathrm{W^2}(m, d) - \eta (\hat{y}(m)-1)y(m)h(d)
\end{align*}
where $\eta$ is the learning rate.

\begin{shaded}
    \question \textbf{Dropout and Regularization}
    
    Dropout is a well-known way to prevent neural networks from overfitting. In this section, you will show this regularization explicitly for linear regression. Recall that linear regression optimizes $w \in \mathbb{R}^{d}$ to minimize the following MSE objective:
    \[
    \mathcal{L}(w)=\|y-\mathrm{X} w\|^{2}
    \]
    where $y \in \mathbb{R}^{n}$ is the response to the design matrix $\mathrm{X} \in \mathbb{R}^{n \times d}$. One way to use dropout during training on the $d$-dimensional input features $x_{i}$ involves keeping each feature at random with probability $p$ (and zero out it if not kept).
    \begin{enumerate}
        \item[a)] Show that when we apply such dropout, the learning objective becomes
        \[
        \mathcal{L}(w)=\mathbb{E}_{\mathrm{M} \sim \operatorname{Bernoulli}(p)}\|y-(\mathrm{M} \odot \mathrm{X}) w\|^{2}
        \]
        where $\odot$ denotes the element-wise product and $\mathrm{M} \in\{0,1\}^{n \times d}$ is a random mask matrix whose element $m_{i, j}$ have $i . i . d$. Bernoulli distribution with success probability $p$.
        \item[b)] Show that we can manipulate the dropout learning objective to an explicit regularized objective:
        \[
        \mathcal{L}(w)=\|y-p \mathrm{X} w\|^{2}+p(1-p)\|\Gamma w\|^{2}
        \]
        and define a suitable matrix $\Gamma$.
    \end{enumerate}
\end{shaded}

\textbf{SOLUTION:}

\textbf{a)} To incorporate dropout, we multiply the input features \(\mathrm{X}\) element-wise with a mask matrix \(\mathrm{M}\) which has values 1 with probability \(p\) and 0 with probability \(1-p\). This randomly masks out features during training. Now, let's compute the objective function:
\[
\begin{aligned}
\mathcal{L}(w) &= \|y - (\mathrm{M} \odot \mathrm{X})w\|^2 \\
&= (y - (\mathrm{M} \odot \mathrm{X})w)^T(y - (\mathrm{M} \odot \mathrm{X})w) \\
&= y^Ty - y^T(\mathrm{M} \odot \mathrm{X})w - w^T(\mathrm{M} \odot \mathrm{X})^Ty + w^T(\mathrm{M} \odot \mathrm{X})^T(\mathrm{M} \odot \mathrm{X})w \\
&= y^Ty - 2w^T(\mathrm{M} \odot \mathrm{X})^Ty + w^T(\mathrm{M} \odot \mathrm{X})^T(\mathrm{M} \odot \mathrm{X})w \\
\end{aligned}
\]

Now, we take the expectation over the mask matrix \(\mathrm{M}\) drawn from the Bernoulli distribution:
\[
\begin{aligned}
\mathcal{L}(w) &= \mathbb{E}_{\mathrm{M} \sim \operatorname{Bernoulli}(p)}[y^Ty - 2w^T(\mathrm{M} \odot \mathrm{X})^Ty + w^T(\mathrm{M} \odot \mathrm{X})^T(\mathrm{M} \odot \mathrm{X})w] \\
&= \mathbb{E}_{\mathrm{M} \sim \operatorname{Bernoulli}(p)}[y^Ty] - 2w^T \mathbb{E}_{\mathrm{M} \sim \operatorname{Bernoulli}(p)}[(\mathrm{M} \odot \mathrm{X})^Ty] + w^T \mathbb{E}_{\mathrm{M} \sim \operatorname{Bernoulli}(p)}[(\mathrm{M} \odot \mathrm{X})^T(\mathrm{M} \odot \mathrm{X})]w \\
&= \|y - \mathbb{E}_{\mathrm{M} \sim \operatorname{Bernoulli}(p)}[(\mathrm{M} \odot \mathrm{X})w]\|^2 \\
&= \mathbb{E}_{\mathrm{M} \sim \operatorname{Bernoulli}(p)}\|y - (\mathrm{M} \odot \mathrm{X})w\|^2
\end{aligned}
\]

Therefore, the learning objective with dropout becomes \(\mathcal{L}(w) = \mathbb{E}_{\mathrm{M} \sim \operatorname{Bernoulli}(p)}\|y - (\mathrm{M} \odot \mathrm{X})w\|^2\).

\textbf{b)} Acknowledgement: This answer refers to UC-Berkeley's CS182\cite{noauthor_cs_nodate}. Starting with the dropout learning objective:
\[
\mathcal{L}(w) = \mathbb{E}_{M \sim \operatorname{Bernoulli}(p)}\|y-(\mathrm{M} \odot \mathrm{X}) w\|^{2}
\]

Let $\mathrm{P}=\mathrm{M} \odot \mathrm{X}$ where $\odot$ is the element-wise multiplication. Therefore, we have:
$$
\|y-\mathrm{P} w\|_{2}^{2}=y^{T} y-2 w^{T} \mathrm{P}^{T} y+w^{T} \mathrm{P}^{T} \mathrm{P} w
$$

That is:
$$
\mathbb{E}_{\mathrm{M} \sim \operatorname{Bernoulli}(p)}\left[\|y-\mathrm{M} \odot \mathrm{X} w\|^{2}\right]=\mathbb{E}_{\mathrm{M}}\left[y^{T} y-2 w^{T} \mathrm{P}^{T} y+w^{T} \mathrm{P}^{T} \mathrm{P} w\right]
$$

Since the expected value of a matrix is the matrix of the expected value of its elements, we have that
$$
\mathbb{E}_{\mathrm{M}}[\mathrm{P}]_{i j}=\mathbb{E}_{\mathrm{M}}\left[(\mathrm{M} \odot \mathrm{X})_{i j}\right]=\mathrm{X}_{i j} \mathbb{E}_{\mathrm{M}}\left[\mathrm{M}_{i j}\right]=p \mathrm{X}_{i j}
$$

It follows that:
$$
\mathbb{E}_{\mathrm{M}}\left[2 w^{T} \mathrm{P}^{T} y\right]=2 p w^{T} \mathrm{X}^{T} y
$$

and:
$$
\left(\mathbb{E}_{\mathrm{M}}\left[\left(\mathrm{P}^{T} \mathrm{P}\right)\right]\right)_{i j}=\Sigma_{k=1}^{n} \mathbb{E}_{\mathrm{M}}\left[\mathrm{M}_{k i} \mathrm{M}_{k j} \mathrm{X}_{k i} \mathrm{X}_{k j}\right]
$$

where:

$\mathbb{E}_{\mathrm{M}}\left[\left(\mathrm{P}^{T} \mathrm{P}\right)\right]_{i j}= \begin{cases}\sum_{k=1}^{n} \mathbb{E}_{\mathrm{M}}\left[\mathrm{M}_{k i} \mathrm{M}_{k j} \mathrm{X}_{k i} \mathrm{X}_{k j}\right]=\sum_{k=1}^{n} \mathbb{E}_{\mathrm{M}}\left[\mathrm{M}_{k i}\right] \mathbb{E}_{\mathrm{M}}\left[\mathrm{M}_{k j}\right] \mathrm{X}_{k i} \mathrm{X}_{k j}=p^{2}\left(\mathrm{X}^{T} \mathrm{X}\right)_{i j} & \text { if } i \neq j \\ \sum_{k=1}^{n} \mathbb{E}_{\mathrm{M}}\left[\mathrm{M}_{k i}^{2} \mathrm{X}_{k i} \mathrm{X}_{k j}\right]=\sum_{k=1}^{n} \mathbb{E}_{\mathrm{M}}\left[\mathrm{M}_{k i}^{2}\right] \mathrm{X}_{k i} \mathrm{X}_{k j}=p\left(\mathrm{X}^{T} \mathrm{X}\right)_{i j} & \text { if } i=j\end{cases}$

Finally, we note that:
$$
\left(\mathbb{E}_{\mathrm{M}}\left[\left(\mathrm{P}^{T} \mathrm{P}\right)\right]\right)_{i j}-p^{2}\left(\mathrm{X}^{T} \mathrm{X}\right)_{i j}= \begin{cases}0 & \text { if } i \neq j \\ \left(p-p^{2}\right)\left(\mathrm{X}^{T} \mathrm{X}\right)_{i j} & \text { if } i=j\end{cases}
$$

We now can put everything together as follow:
$$
\begin{aligned}
& \mathcal{L}(w)=\mathbb{E}_{\mathrm{M}}\left[\|y-\mathrm{M} \odot \mathrm{X} w\|^{2}\right] \\
& =\mathbb{E}_{\mathrm{M}}\left[y^{T} y-2 w^{T} \mathrm{P}^{T} y+w^{T} \mathrm{P}^{T} \mathrm{P} w\right] \\
& =y^{T} y-2 p w^{T} \mathrm{X}^{T} y+p^{2} w^{T} \mathrm{X}^{T} \mathrm{X} w-p^{2} w^{T} \mathrm{X}^{T} \mathrm{X} w+w^{T} \mathbb{E}_{\mathrm{M}}\left[\mathrm{P}^{T} \mathrm{P}\right] w \\
& =\|y-p \mathrm{X} w\|^{2}+\left(w^{T} \mathbb{E}_{\mathrm{M}}\left[\mathrm{P}^{T} \mathrm{P}\right] w-p^{2} w^{T} \mathrm{X}^{T} \mathrm{X} w\right) \\
& =\|y-p \mathrm{X} w\|^{2}+w^{T}\left(\mathbb{E}_{\mathrm{M}}\left[\mathrm{P}^{T} \mathrm{P}\right] -p^{2}\left(\mathrm{X}^{T} \mathrm{X}\right)\right) w \\
& =\|y-p \mathrm{X} w\|^{2}+\left(p^{2}-p\right) w^{T}\left(\operatorname{diag}\left(\mathrm{X}^{T} \mathrm{X}\right)\right) w \\
& =\|y-p \mathrm{X} w\|^{2}+p(1-p) w^{T}\left(\operatorname{diag}\left(\mathrm{X}^{T} \mathrm{X}\right)\right) w \\
& =\|y-p \mathrm{X} w\|^{2}+p(1-p)\|\Gamma w\|^{2}
\end{aligned}
$$

where $\operatorname{diag}\left(\mathrm{X}^{T} \mathrm{X}\right)$ refers to the matrix where the non-diagonal elements of $\mathrm{X}^{T} \mathrm{X}$ are set to 0, and $\Gamma=\left(\operatorname{diag}\left(\mathrm{X}^{T} \mathrm{X}\right)\right)^{1 / 2}$, which exists as $\mathrm{X}^{T} \mathrm{X}$ is positive-semidefinite(PSD) and therefore has non-negative diagonal elements.

% \rule[0ex]{\textwidth}{0.5pt}
\bibliographystyle{IEEEtran}
\bibliography{citations}

\end{document}